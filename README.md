# Avivo Mini-RAG Telegram Bot

A lightweight Retrieval-Augmented Generation (RAG) chatbot that answers knowledge base questions via Telegram. Combines Python, FAISS for vector search, sentence-transformers for embeddings, a fast LLM API (OpenAI or Hugging Face), and an interactive Telegram interface.

---

![App Screenshot](Screenshot_1.png)

![App Screenshot](Screenshot_2.png)


## ğŸ“Œ Features

- ğŸ” Mini-RAG system: FAISS + sentence-transformers for top-k retrieval
- ğŸ¤– Telegram bot: `/ask`, `/summarize`, `/help`, `/start` commands
- ğŸ“š Markdown knowledge base (company policies, recipes, FAQs)
- âš¡ Efficient local FAISS index for instant look-up
- ğŸ§  Modern LLM API (OpenAI or HuggingFace Inference) for answer generation
- ğŸ—‚ï¸ Shows which document sources were used in every answer
- ğŸ“ Maintains user session history (last 3 queries) for summarization
- ğŸ“¦ Clean, modular codebase (`src/bot.py`, `src/rag_core.py`)
- ğŸ–¼ï¸ Demo screenshots included

---

## ğŸ§° Tech Stack & Components Used:



| Category           | Name / Version                                    | Purpose                                   |
|--------------------|---------------------------------------------------|-------------------------------------------|
| Language           | Python 3.10+                                      | Core implementation                       |
| Embeddings Model   | sentence-transformers/all-MiniLM-L6-v2            | Text chunk embedding for RAG              |
| Vector Store       | FAISS                                              | Efficient similarity search               |
| LLM (Inference)    | Meta Llama-3 3B Instruct via HuggingFace Inference| Response generation for retrieved context |
| Tokenizer          | HuggingFace Tokenizers                            | Used internally by HF models              |
| RAG Pipeline       | Custom retrieval + generation logic               | Chunking, embedding, search, synthesis    |
| Telegram Integration| python-telegram-bot                              | Chatbot interface                         |
| Serialization      | Pickle (.pkl) files                               | Embedding cache, metadata, chunks         |
| Environment        | .env + python-dotenv                              | API key management                        |
| Utilities          | tqdm, dotenv, os, pickle                          | Helper functionality                      |

---

## ğŸ—ï¸ System Architecture

```

                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       Telegram User      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     bot.py         â”‚
                    â”‚ Telegram Interface â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    rag_core.py     â”‚
                     â”‚  RAG Pipeline      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   FAISS Index      â”‚        â”‚    LLM Inference API    â”‚
    â”‚   (vector store)   â”‚        â”‚(OpenAI/Hugging Face LLM)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚   Top-k chunks                 â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Final Answer    â”‚
              â”‚    + Sources       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Telegram Response  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
```

## ğŸ“ Project Structure

```

avivo-mini-rag-bot/
â”‚
â”œâ”€â”€ kb/
â”‚   â”œâ”€â”€ code.ipynb               \# Development notebook
â”‚   â”œâ”€â”€ faiss_index.bin          \# FAISS vector index
â”‚   â”œâ”€â”€ metadata.pkl             \# Metadata for chunks
â”‚   â”œâ”€â”€ faq.md                   \# Example markdown knowledge file
â”‚   â”œâ”€â”€ policy.md
â”‚   â”œâ”€â”€ recipes.md
â”‚   â”œâ”€â”€ kb_chunks.pkl
â”‚   â””â”€â”€ embed_cache.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bot.py                   \# Telegram bot logic
â”‚   â””â”€â”€ rag_core.py              \# RAG core pipeline
â”‚
â”œâ”€â”€ .env                         \# API keys/secrets (not versioned)
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ DataScience_Assignment.docx   \# Problem statement (if provided)
â”œâ”€â”€ Screenshot_1.png              \# Demo: /ask query
â””â”€â”€ Screenshot_2.png              \# Demo: /summarize

```

---

## âš™ï¸ Setup Instructions

**1. Extract the project:**
Download and extract the provided avivo-mini-rag-bot.zip file into any folder.
```

avivo-mini-rag-bot/


```

**2. Create a virtual environment (recommended):**
```

python -m venv venv

# macOS/Linux:
source venv/bin/activate

# Windows:
venv\Scripts\activate

```

**3. Install dependencies:**
```

pip install -r requirements.txt

```

**4. Configure environment variables:**
Create a file `.env` in the root folder with:
```

TELEGRAM_BOT_TOKEN=your_telegram_token
HUGGINGFACEHUB_API_TOKEN=your_hf_token

```

**5. Run the bot:**
```

python src/bot.py

```

---

## ğŸ§  RAG Details

- **Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2`
    - Fast, accurate, small memory
- **Vector Retriever:** `FAISS`
    - Fast local similarity search
- **LLM (Answer Generation):**
    -  HuggingFace Inference (reliable, simple)
- **Modularity:** All RAG logic in `rag_core.py`, bot logic in `bot.py`. Easy to swap out LLM backends.

---

## ğŸ§ª Commands

| Command      | Description                     |
|--------------|---------------------------------|
| `/ask <q>`   | Ask a question from the KB      |
| `/summarize` | Summarize last answer           |
| `/help`      | Bot usage instructions          |
| `/start`     | Welcome message                 |

---

## ğŸ–¼ï¸ Demo Screenshots

| Screenshot         | Description               |
|--------------------|--------------------------|
| Screenshot_1.png   | Bot answering `/ask`     |
| Screenshot_2.png   | Bot answering `/summarize`|

---

## ğŸ¯ Evaluation 

| Area           | How This Project Addresses It                                            |
|----------------|-------------------------------------------------------------------------|
| Code Quality   | Clean, modular structure (`rag_core.py`, `bot.py`), minimal dependencies.|
| System Design  | Clear flow: User â†’ Bot â†’ RAG â†’ FAISS â†’ LLM Model â†’ Response.             |
| Model Use      | Lightweight models (MiniLM + LLM API) chosen for speed and simplicity.   |
| Efficiency     | Cached embeddings, FAISS index, top-k retrieval keep responses fast.      |
| User Experience| Clear answers, source references, `/summarize` support, simple commands. |
| Innovation     | Adds per-user history + summarization for enhanced interaction.           |


## ğŸ¯ Conclusion

This project fulfills the assignment with:
- A true Mini-RAG pipeline
- Telegram bot interface
- Source-based answers & summaries
- Fast FAISS retrieval
- Open, clean, modular code
- Configurable LLM backend (HF)
